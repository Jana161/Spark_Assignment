pip install confluent-kafka

from confluent_kafka import Producer

def delivery_report(err, msg):
    if err is not None:
        print('Message delivery failed: {}'.format(err))
    else:
        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))

producer = Producer({'bootstrap.servers': 'your_kafka_server'})

# Define your Kafka topic
topic = 'your_topic'

# Read data from your files and produce it to Kafka
with open('file1.csv', 'r') as file1, open('file2.csv', 'r') as file2, open('file3.csv', 'r') as file3:
    for line in file1:
        producer.produce(topic, key=None, value=line, callback=delivery_report)
    for line in file2:
        producer.produce(topic, key=None, value=line, callback=delivery_report)
    for line in file3:
        producer.produce(topic, key=None, value=line, callback=delivery_report)

producer.flush()


from confluent_kafka import Consumer, KafkaError
from pymongo import MongoClient

# Set up Kafka consumer
consumer = Consumer({
    'bootstrap.servers': 'your_kafka_server',
    'group.id': 'your_consumer_group',
    'auto.offset.reset': 'earliest'
})
consumer.subscribe(['your_topic'])

# Set up MongoDB connection
client = MongoClient('your_mongodb_connection_string')
db = client['your_database']
collection = db['your_collection']

# Consume and insert data into MongoDB
while True:
    msg = consumer.poll(1.0)

    if msg is None:
        continue
    if msg.error():
        if msg.error().code() == KafkaError._PARTITION_EOF:
            print('Reached end of partition')
        else:
            print('Error: {}'.format(msg.error()))
    else:
        data = msg.value()
        # Assuming data is in JSON format, you can parse it and insert it into MongoDB
        collection.insert_one(data)

# Close connections when done
consumer.close()
client.close()


# Register DataFrames as temporary tables
case_df.createOrReplaceTempView("cases_table")

# Run SQL queries
result_df = spark.sql('SELECT * FROM cases_table WHERE confirmed > 100')
result_df.show()

o create a Spark UDF, you can define a Python function and then register it as a UDF. Here's an example:

python
Copy code
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

def case_high_low(confirmed):
    if confirmed < 50:
        return 'low'
    else:
        return 'high'

# Register UDF
case_high_low_udf = udf(case_high_low, StringType())

# Apply UDF to a DataFrame column
case_df = case_df.withColumn("high_low", case_high_low_udf(case_df["confirmed"]))
